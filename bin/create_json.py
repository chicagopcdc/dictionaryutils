""" Combine YAML schema files into JSON dictionary file for load into data portal """
# pylint: disable=line-too-long
import json
import os
import shutil
import sys
import typing

from dotenv import load_dotenv

from utils import add_codes, add_enum_description
from dictionaryutils import dump_schemas_from_dir


try:
    os.mkdir("../artifacts")
except OSError:
    pass

# Load env variables
load_dotenv('.env')

# path to the datadictionary/gdcdictionary/schemas/ folder
schema_path: str = os.environ.get("SCHEMA_PATH", "../../gdcdictionary/schemas/")

table_name_mapping: dict[str, str] = {
    "adverse_event": "adverse_events",
    "biopsy_surgical_procedure": "biopsy_and_surgical_procedures",
    "disease_characteristic": "disease_characteristics",
    "lab": "laboratory_test",
    "late_effect": "late_effects",
    "lesion_characteristic": "lesion_characteristics",
    "molecular_analysis": "genetic_analysis",
    "myeloid_sarcoma_involvement": "disease_characteristics",
    "non_protocol_therapy": "medication",
    "off_protocol_therapy_study": "off_protocol_therapy_or_study",
    "person": "demographics",
    "protocol_treatment_modification": "protocol_treatment_modifications",
    "study": "subject_characteristics",
    "subject": "subject_characteristics",
    "survival_characteristic": "survival_characteristics",
    "total_dose": "medication",
    "tumor_assessment": "tumor_characteristics"
}

# make sure timing.yaml occurs first in output schema json
yaml_schemas: dict[str, any] = dump_schemas_from_dir(schema_path)
yaml_schemas_with_timing: dict[str, any] = {k:v for k,v in yaml_schemas.items() if k == "timing.yaml"}
yaml_schemas_without_timing: dict[str, any] = {k:v for k,v in yaml_schemas.items() if k != "timing.yaml"}
yaml_schemas = {**yaml_schemas_with_timing, **yaml_schemas_without_timing}


# Add NCI concept code details from all_variables.json file generated by the pcdcmodel repo
# (https://github.com/chicagopcdc/pcdcmodels) Michael Watkins is working on.
# TODO: the all_variables.json file will eventually be used to generate all of the yaml files in
# datadictionary/gdcdictionary/schemas programmatically. It is not ready to fully replace this manual curation
# script yet but it can be leveraged to populate details for enums that would otherwise be long/time-consuming
excluded_variables: list[str] = [
    "created_datetime",
    "id",
    "persons",
    "project_id",
    "projects",
    "state",
    "subjects",
    "submitter_id",
    "timings",
    "type",
    "updated_datetime"
]
all_vars: dict[str, dict[str, any]] = {}
fd_all_vars: typing.TextIO
with open("./all_variables.json", encoding="utf-8") as fd_all_vars:
    all_vars_raw: dict[str, any] = json.load(fd_all_vars)

    # START PATCH
    # The JSON DD (all_variables.json) groups the tables by higher level categories
    # e.g. 'events.adverse_events', 'treatment.radiation_therapy', etc.
    # I (Luca) checked with Michael, this are just for easiness when talking with consortia.
    # We Should generate a file without the grouping to avoid parsing the key and making changes here
    tbl_dot_var: str
    cat_tbl_var: str
    var_metadata: any
    for cat_tbl_var, var_metadata in all_vars_raw.items():
        tbl_dot_var = cat_tbl_var.partition('.')[2].lower()
        if not tbl_dot_var or tbl_dot_var.count('.') != 1:
            # skip categories that aren't in 'category.table.VARIABLE' format e.g. 'table guidance'
            continue
        all_vars[tbl_dot_var] = var_metadata
    # END PATCH

    yaml_file_name: str
    yaml_file_content: dict[str, any]
    for yaml_file_name, yaml_file_content in yaml_schemas.items():
        print(f"Processing '{yaml_file_name}'")
        if "properties" not in yaml_file_content:
            print(f"WARNING: file/table {yaml_file_name} is missing the properties value")
            continue

        table_name: str = yaml_file_name.split(".")[0]
        table_name = table_name_mapping.get(table_name, table_name)

        variable_name: str
        variable_values: any
        for variable_name, variable_values in yaml_file_content["properties"].items():
            if variable_name in excluded_variables:
                continue

            tbl_dot_var = table_name + "." + variable_name
            if tbl_dot_var not in all_vars:
                print(f"WARNING: variable '{tbl_dot_var}' not found in all_variables.json file")
                continue

            add_codes(all_vars[tbl_dot_var], yaml_schemas["_terms.yaml"], variable_name, "term", variable_values)

            # TODO need to finish doing the bulk load of the terms before re-enabling this,
            # otherwise the generation of the schema.json will take a very long time
            if "values" in all_vars[tbl_dot_var]:
                value_key: str
                value_value: any
                for value_key, value_value in all_vars[tbl_dot_var]["values"].items():
                    add_codes(
                        value_value,
                        yaml_schemas["_terms.yaml"],
                        variable_name,
                        "enumDef",
                        variable_values,
                        value_key
                    )

                    # TODO find a way to generalize this - may need to add a variable in the all_variables.json
                    # file containing the necessary info. For now we are only interested in consortium to have
                    # this extra diseases description
                    if variable_name == "consortium":
                        print(f"adding consortium enum description for '{tbl_dot_var}' ({value_key})")
                        add_enum_description(variable_values, value_value, value_key)

# Save files
schema_file_dir: str = "../artifacts"
schema_file_name: str = "schema.json"
with open(os.path.join(schema_file_dir, schema_file_name), "w", encoding="utf-8") as f:
    json.dump(yaml_schemas, f)

# Copy to individual environment files
if len(sys.argv) == 2:
    project_code: str = sys.argv[1] # e.g. 20240130
    env_names: tuple[str, ...] = ("demo", "dev", "staging", "prod")
    env_name: str
    for env_name in env_names:
        shutil.copyfile(
            os.path.join(schema_file_dir, schema_file_name),
            os.path.join(schema_file_dir, f"pcdc-schema-{env_name}-{project_code}.json")
        )
    os.remove(os.path.join(schema_file_dir, schema_file_name))
else:
    print('*** Specify project code (yyyymmdd) as arg to create dictionary file for envs instead of schema.json ***')

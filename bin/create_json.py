""" Combine YAML schema files into JSON dictionary file for load into data portal """
# pylint: disable=line-too-long:
import json
import os
import shutil
import sys

from dotenv import load_dotenv

from dictionaryutils import dump_schemas_from_dir
from utils import add_codes, add_enum_description


try:
    os.mkdir("../artifacts")
except OSError:
    pass

# Load env variables
load_dotenv('.env')

# path to the datadictionary/gdcdictionary/schemas/ folder
schema_path: str = os.environ.get("SCHEMA_PATH", "../../gdcdictionary/schemas/")

table_name_mapping: dict[str, str] = {
    "adverse_event": "adverse_events",
    "biopsy_surgical_procedure": "biopsy_and_surgical_procedures",
    "disease_characteristic": "disease_characteristics",
    "lab": "laboratory_test",
    "late_effect": "late_effects",
    "lesion_characteristic": "lesion_characteristics",
    "molecular_analysis": "genetic_analysis",
    "myeloid_sarcoma_involvement": "disease_characteristics",
    "non_protocol_therapy": "medication",
    "off_protocol_therapy_study": "off_protocol_therapy_or_study",
    "person": "demographics",
    "protocol_treatment_modification": "protocol_treatment_modifications",
    "study": "subject_characteristics",
    "subject": "subject_characteristics",
    "survival_characteristic": "survival_characteristics",
    "total_dose": "medication",
    "tumor_assessment": "tumor_characteristics"
}

# make sure timing.yaml occurs first in output schema json
yaml_schemas: dict[str, any] = dump_schemas_from_dir(schema_path)
yaml_schemas_with_timing: dict[str, any] = {k:v for k,v in yaml_schemas.items() if k == "timing.yaml"}
yaml_schemas_without_timing: dict[str, any] = {k:v for k,v in yaml_schemas.items() if k != "timing.yaml"}
yaml_schemas = {**yaml_schemas_with_timing, **yaml_schemas_without_timing}


# Add NCIt codes from all_variables.json artifact generated by the pcdcmodel repo Michael is working on
# TODO the artifact from that repo will eventually be used to generate all the yaml file programmatically.
# It is not ready to fully replace this manual curation but it can be leveraged to populate long and time consuming enums
json_dd: str = "./all_variables.json"
excluded_variables: list[str] = [
    "created_datetime",
    "id",
    "persons",
    "project_id",
    "projects",
    "state",
    "subjects",
    "submitter_id",
    "timings",
    "type",
    "updated_datetime"
]
with open(json_dd, encoding="utf-8") as dd_file:
    dd_file_json_grouped: dict[str, any] = json.load(dd_file)

    # START PATCH
    # The JSON DD groups the tables by higher level categories.
    # I checked with Michael, this are just for easiness when talking with consortia.
    # We Should generate a file without the grouping to avoid parsing the key and making changes here
    dd_file_json: dict[str, any] = {}
    category: str
    table_value: any
    for category, table_value in dd_file_json_grouped.items():
        if category == "table guidance":
            continue
        dd_file_json[(category[category.index(".") + 1:len(category)]).lower()] = table_value
    # END PATCH

    table_name: str
    table_content: any
    for table_name, table_content in yaml_schemas.items():
        if "properties" not in table_content:
            print(f"WARNING: file/table {table_name} is missing the properties value.")
            continue

        variable_name: str
        variable_values: any
        for variable_name, variable_values in table_content["properties"].items():
            if variable_name in excluded_variables:
                continue

            composite_name = table_name.split(".")[0] + "." + variable_name
            if composite_name not in dd_file_json:
                print(f"WARNING: variable {composite_name} is missing the properties value.")
                if table_name.split(".")[0] in table_name_mapping:
                    composite_name = table_name_mapping[table_name.split(".")[0]] + "." + variable_name
                    print("trying mapping for " + composite_name)
                    if composite_name not in dd_file_json:
                        print(f"WARNING: variable {composite_name} is missing the properties value.")
                        continue
                else:
                    print(f"WARNING: Missing mapping for {composite_name}")
                    continue

            add_codes(dd_file_json[composite_name], yaml_schemas["_terms.yaml"], variable_name, "term", variable_values)

            # TODO need to finish doing the bulk load of the terms before re-enabling this,
            # otherwise the generation of the schema.json will take a very long time
            if "values" in dd_file_json[composite_name]:
                value_key: str
                value_value: any
                for value_key, value_value in dd_file_json[composite_name]["values"].items():
                    add_codes(
                        value_value,
                        yaml_schemas["_terms.yaml"],
                        variable_name,
                        "enumDef",
                        variable_values,
                        value_key
                    )

                    # TODO find a way to generalize this - may need to add a variable in the all_variables.json
                    # file containing the necessary info. For now we are only interested in consortium to have
                    # this extra diseases description
                    if variable_name == "consortium":
                        add_enum_description(variable_values, value_value, value_key)

# Save files
schema_file_dir: str = "../artifacts"
schema_file_name: str = "schema.json"
with open(os.path.join(schema_file_dir, schema_file_name), "w", encoding="utf-8") as f:
    json.dump(yaml_schemas, f)

# Copy to individual environment files
if len(sys.argv) == 2:
    project_code: str = sys.argv[1] # e.g. 20240130
    env_names: tuple[str, ...] = ("demo", "dev", "staging", "prod")
    env_name: str
    for env_name in env_names:
        shutil.copyfile(
            os.path.join(schema_file_dir, schema_file_name),
            os.path.join(schema_file_dir, f"pcdc-schema-{env_name}-{project_code}.json")
        )
    os.remove(os.path.join(schema_file_dir, schema_file_name))
else:
    print('*** Specify project code (yyyymmdd) as arg to create dictionary file for envs instead of schema.json ***')
